{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of CUDA devices: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Number of CUDA devices:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct  2 09:43:14 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:4A:00.0 Off |                    0 |\n",
      "| N/A   36C    P8    14W /  70W |      5MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   35C    P8    14W /  70W |      5MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parallel-cnn.pt',\n",
       " 'cnn.pth',\n",
       " 'parallel-cnn.ipynb',\n",
       " 'parallel-cnn.py',\n",
       " 'cnn.py',\n",
       " 'data',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fs03/vf38/msyukron/data has been deleted!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_data():\n",
    "    data_dir = '/fs03/vf38/msyukron/data'\n",
    "    if os.path.exists(data_dir):\n",
    "        shutil.rmtree(data_dir)\n",
    "        print(f\"{data_dir} has been deleted!\")\n",
    "    else:\n",
    "        print(f\"{data_dir} does not exist!\")\n",
    "              \n",
    "delete_data()\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Remove corrupted CIFAR-10 data directory\n",
    "data_dir = '/fs03/vf38/msyukron/data'\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /fs03/vf38/msyukron/cnn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /fs03/vf38/msyukron/cnn.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(128*2*2, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 128*2*2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def load_train_objs():\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    train_set = datasets.CIFAR10(root='/fs03/vf38/msyukron/data', train=True, download=True, transform=transform)\n",
    "    model = SimpleCNN()  \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    return train_set, model, optimizer\n",
    "\n",
    "def train(model, train_loader, optimizer, epochs, save_every, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(\"Epoch {}, Loss: {:.4f}\".format(epoch+1, running_loss / len(train_loader)))\n",
    "        \n",
    "        # Save the model every 'save_every' epochs\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            PATH = '/fs03/vf38/msyukron/cnn.pth'\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            print(\"Model saved at {}\".format(PATH))\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Train a CNN on CIFAR10')\n",
    "    parser.add_argument('epochs', type=int, help='Number of epochs to train')\n",
    "    parser.add_argument('save_every', type=int, help='Save the model every X epochs')\n",
    "    parser.add_argument('--batch_size', default=32, type=int, help='Batch size for training')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    train_set, model, optimizer = load_train_objs()\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
    "    train(model, train_loader, optimizer, args.epochs, args.save_every, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /fs03/vf38/msyukron/data/cifar-10-python.tar.gz\n",
      "100%|██████████████████████▉| 170303488/170498071 [00:13<00:00, 14051718.65it/s]Extracting /fs03/vf38/msyukron/data/cifar-10-python.tar.gz to /fs03/vf38/msyukron/data\n",
      "Epoch 1, Loss: 2.3030\n",
      "170500096it [00:30, 14051718.65it/s]                                            Epoch 2, Loss: 2.3028\n",
      "Epoch 3, Loss: 2.3027\n",
      "Epoch 4, Loss: 2.3025\n",
      "Epoch 5, Loss: 2.3024\n",
      "Epoch 6, Loss: 2.3023\n",
      "Epoch 7, Loss: 2.3022\n",
      "Epoch 8, Loss: 2.3021\n",
      "Epoch 9, Loss: 2.3020\n",
      "Epoch 10, Loss: 2.3019\n",
      "Epoch 11, Loss: 2.3018\n",
      "Epoch 12, Loss: 2.3017\n",
      "Epoch 13, Loss: 2.3016\n",
      "Epoch 14, Loss: 2.3015\n",
      "Epoch 15, Loss: 2.3014\n",
      "Epoch 16, Loss: 2.3012\n",
      "Epoch 17, Loss: 2.3011\n",
      "Epoch 18, Loss: 2.3010\n",
      "Epoch 19, Loss: 2.3008\n",
      "Epoch 20, Loss: 2.3006\n",
      "Epoch 21, Loss: 2.3004\n",
      "Epoch 22, Loss: 2.3001\n",
      "Epoch 23, Loss: 2.2998\n",
      "Epoch 24, Loss: 2.2995\n",
      "Epoch 25, Loss: 2.2991\n",
      "Epoch 26, Loss: 2.2987\n",
      "Epoch 27, Loss: 2.2981\n",
      "Epoch 28, Loss: 2.2975\n",
      "Epoch 29, Loss: 2.2968\n",
      "Epoch 30, Loss: 2.2959\n",
      "Epoch 31, Loss: 2.2948\n",
      "Epoch 32, Loss: 2.2934\n",
      "Epoch 33, Loss: 2.2917\n",
      "Epoch 34, Loss: 2.2894\n",
      "Epoch 35, Loss: 2.2862\n",
      "Epoch 36, Loss: 2.2820\n",
      "Epoch 37, Loss: 2.2761\n",
      "Epoch 38, Loss: 2.2677\n",
      "Epoch 39, Loss: 2.2559\n",
      "Epoch 40, Loss: 2.2393\n",
      "Epoch 41, Loss: 2.2164\n",
      "Epoch 42, Loss: 2.1853\n",
      "Epoch 43, Loss: 2.1444\n",
      "Epoch 44, Loss: 2.0990\n",
      "Epoch 45, Loss: 2.0615\n",
      "Epoch 46, Loss: 2.0359\n",
      "Epoch 47, Loss: 2.0176\n",
      "Epoch 48, Loss: 2.0005\n",
      "Epoch 49, Loss: 1.9814\n",
      "Epoch 50, Loss: 1.9610\n",
      "Model saved at /fs03/vf38/msyukron/cnn.pth\n",
      "Epoch 51, Loss: 1.9377\n",
      "Epoch 52, Loss: 1.9128\n",
      "Epoch 53, Loss: 1.8878\n",
      "Epoch 54, Loss: 1.8641\n",
      "Epoch 55, Loss: 1.8431\n",
      "Epoch 56, Loss: 1.8235\n",
      "Epoch 57, Loss: 1.8057\n",
      "Epoch 58, Loss: 1.7894\n",
      "Epoch 59, Loss: 1.7744\n",
      "Epoch 60, Loss: 1.7600\n",
      "Epoch 61, Loss: 1.7455\n",
      "Epoch 62, Loss: 1.7322\n",
      "Epoch 63, Loss: 1.7179\n",
      "Epoch 64, Loss: 1.7037\n",
      "Epoch 65, Loss: 1.6885\n",
      "Epoch 66, Loss: 1.6768\n",
      "Epoch 67, Loss: 1.6633\n",
      "Epoch 68, Loss: 1.6514\n",
      "Epoch 69, Loss: 1.6380\n",
      "Epoch 70, Loss: 1.6264\n",
      "Epoch 71, Loss: 1.6140\n",
      "Epoch 72, Loss: 1.6019\n",
      "Epoch 73, Loss: 1.5915\n",
      "Epoch 74, Loss: 1.5793\n",
      "Epoch 75, Loss: 1.5685\n",
      "Epoch 76, Loss: 1.5583\n",
      "Epoch 77, Loss: 1.5470\n",
      "Epoch 78, Loss: 1.5390\n",
      "Epoch 79, Loss: 1.5275\n",
      "Epoch 80, Loss: 1.5192\n",
      "Epoch 81, Loss: 1.5107\n",
      "Epoch 82, Loss: 1.5020\n",
      "Epoch 83, Loss: 1.4927\n",
      "Epoch 84, Loss: 1.4839\n",
      "Epoch 85, Loss: 1.4742\n",
      "Epoch 86, Loss: 1.4669\n",
      "Epoch 87, Loss: 1.4591\n",
      "Epoch 88, Loss: 1.4507\n",
      "Epoch 89, Loss: 1.4424\n",
      "Epoch 90, Loss: 1.4350\n",
      "Epoch 91, Loss: 1.4261\n",
      "Epoch 92, Loss: 1.4170\n",
      "Epoch 93, Loss: 1.4086\n",
      "Epoch 94, Loss: 1.4019\n",
      "Epoch 95, Loss: 1.3943\n",
      "Epoch 96, Loss: 1.3842\n",
      "Epoch 97, Loss: 1.3765\n",
      "Epoch 98, Loss: 1.3704\n",
      "Epoch 99, Loss: 1.3601\n",
      "Epoch 100, Loss: 1.3538\n",
      "Model saved at /fs03/vf38/msyukron/cnn.pth\n",
      "Epoch 101, Loss: 1.3456\n",
      "Epoch 102, Loss: 1.3373\n",
      "Epoch 103, Loss: 1.3296\n",
      "Epoch 104, Loss: 1.3210\n",
      "Epoch 105, Loss: 1.3140\n",
      "Epoch 106, Loss: 1.3054\n",
      "Epoch 107, Loss: 1.2987\n",
      "Epoch 108, Loss: 1.2915\n",
      "Epoch 109, Loss: 1.2833\n",
      "Epoch 110, Loss: 1.2763\n",
      "Epoch 111, Loss: 1.2684\n",
      "Epoch 112, Loss: 1.2616\n",
      "Epoch 113, Loss: 1.2535\n",
      "Epoch 114, Loss: 1.2464\n",
      "Epoch 115, Loss: 1.2417\n",
      "Epoch 116, Loss: 1.2329\n",
      "Epoch 117, Loss: 1.2277\n",
      "Epoch 118, Loss: 1.2203\n",
      "Epoch 119, Loss: 1.2131\n",
      "Epoch 120, Loss: 1.2057\n",
      "Epoch 121, Loss: 1.2008\n",
      "Epoch 122, Loss: 1.1948\n",
      "Epoch 123, Loss: 1.1870\n",
      "Epoch 124, Loss: 1.1792\n",
      "Epoch 125, Loss: 1.1747\n",
      "Epoch 126, Loss: 1.1686\n",
      "Epoch 127, Loss: 1.1626\n",
      "Epoch 128, Loss: 1.1539\n",
      "Epoch 129, Loss: 1.1477\n",
      "Epoch 130, Loss: 1.1405\n",
      "Epoch 131, Loss: 1.1348\n",
      "Epoch 132, Loss: 1.1293\n",
      "Epoch 133, Loss: 1.1238\n",
      "Epoch 134, Loss: 1.1170\n",
      "Epoch 135, Loss: 1.1095\n",
      "Epoch 136, Loss: 1.1035\n",
      "Epoch 137, Loss: 1.0985\n",
      "Epoch 138, Loss: 1.0926\n",
      "Epoch 139, Loss: 1.0847\n",
      "Epoch 140, Loss: 1.0800\n",
      "Epoch 141, Loss: 1.0733\n",
      "Epoch 142, Loss: 1.0671\n",
      "Epoch 143, Loss: 1.0617\n",
      "Epoch 144, Loss: 1.0590\n",
      "Epoch 145, Loss: 1.0503\n",
      "Epoch 146, Loss: 1.0449\n",
      "Epoch 147, Loss: 1.0400\n",
      "Epoch 148, Loss: 1.0348\n",
      "Epoch 149, Loss: 1.0285\n",
      "Epoch 150, Loss: 1.0235\n",
      "Model saved at /fs03/vf38/msyukron/cnn.pth\n",
      "Finished Training\n",
      "170500096it [26:13, 108340.15it/s]  \n",
      "\n",
      "real\t26m14.888s\n",
      "user\t25m31.857s\n",
      "sys\t0m54.241s\n"
     ]
    }
   ],
   "source": [
    "!time python3 /fs03/vf38/msyukron/cnn.py 150 50 --batch_size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Predictions:   cat  ship  ship plane  frog  frog   car  frog   cat   car\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(128*2*2, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 128*2*2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load the model\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load('/fs03/vf38/msyukron/cnn.pth'))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load the CIFAR-10 test set\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "test_set = datasets.CIFAR10(root='/fs03/vf38/msyukron/data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=10, shuffle=False)  # Use shuffle=False to get the first 10 samples\n",
    "\n",
    "# Get 10 samples from test set\n",
    "data_iter = iter(test_loader)\n",
    "images, labels = next(data_iter)  # Using the built-in next function\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Predict\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Print the predictions\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print('Predictions:', ' '.join('%5s' % classes[predicted[j]] for j in range(10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fs03/vf38/msyukron/data has been deleted!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_data():\n",
    "    data_dir = '/fs03/vf38/msyukron/data'\n",
    "    if os.path.exists(data_dir):\n",
    "        shutil.rmtree(data_dir)\n",
    "        print(f\"{data_dir} has been deleted!\")\n",
    "    else:\n",
    "        print(f\"{data_dir} does not exist!\")\n",
    "              \n",
    "delete_data()\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Remove corrupted CIFAR-10 data directory\n",
    "data_dir = '/fs03/vf38/msyukron/data'\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /fs03/vf38/msyukron/parallel-cnn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /fs03/vf38/msyukron/parallel-cnn.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import os\n",
    "import time\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(128*2*2, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 128*2*2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def load_train_objs():\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    train_set = datasets.CIFAR10(root='/fs03/vf38/msyukron/data', train=True, download=True, transform=transform)\n",
    "    model = SimpleCNN()  \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    return train_set, model, optimizer\n",
    "\n",
    "def prepare_dataloader(dataset: datasets, batch_size: int):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        sampler=DistributedSampler(dataset)\n",
    "    )\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: torch.nn.Module, train_data: DataLoader, optimizer: torch.optim.Optimizer, gpu_id: int, save_every: int) -> None:\n",
    "        self.gpu_id = gpu_id\n",
    "        self.model = model.to(gpu_id)\n",
    "        self.train_data = train_data\n",
    "        self.optimizer = optimizer\n",
    "        self.save_every = save_every\n",
    "        self.model = DDP(model, device_ids=[gpu_id])\n",
    "\n",
    "    def _run_batch(self, source, targets):\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(source)\n",
    "        loss = F.cross_entropy(output, targets.squeeze().long())  # updated loss function\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def _run_epoch(self, epoch):\n",
    "        b_sz = len(next(iter(self.train_data))[0])\n",
    "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
    "        self.train_data.sampler.set_epoch(epoch)\n",
    "        for source, targets in self.train_data:\n",
    "            source = source.to(self.gpu_id)\n",
    "            targets = targets.to(self.gpu_id)\n",
    "            self._run_batch(source, targets)\n",
    "\n",
    "    def _save_checkpoint(self, epoch):\n",
    "        ckp = self.model.module.state_dict()\n",
    "        PATH = \"/fs03/vf38/msyukron/parallel-cnn.pt\"\n",
    "        torch.save(ckp, PATH)\n",
    "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
    "\n",
    "    def train(self, max_epochs: int):\n",
    "        for epoch in range(max_epochs):\n",
    "            self._run_epoch(epoch)\n",
    "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
    "                self._save_checkpoint(epoch)\n",
    "\n",
    "def main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):\n",
    "    ddp_setup(rank, world_size)\n",
    "    dataset, model, optimizer = load_train_objs()\n",
    "    train_data = prepare_dataloader(dataset, batch_size)\n",
    "    trainer = Trainer(model, train_data, optimizer, rank, save_every)\n",
    "    trainer.train(total_epochs)\n",
    "    destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='simple distributed training job')\n",
    "    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n",
    "    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n",
    "    parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[GPU1] Epoch 0 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 0 | Batchsize: 64 | Steps: 391\n",
      "Epoch 0 | Training checkpoint saved at /fs03/vf38/msyukron/parallel-cnn.pt\n",
      "[GPU1] Epoch 1 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 1 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 2 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 2 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 3 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 3 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 4 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 4 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 5 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 5 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 6 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 6 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 7 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 7 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 8 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 8 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 9 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 9 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 10 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 10 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 11 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 11 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 12 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 12 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 13 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 13 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 14 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 14 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 15 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 15 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 16 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 16 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 17 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 17 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 18 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 18 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 19 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 19 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 20 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 20 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 21 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 21 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 22 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 22 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 23 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 23 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 24 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 24 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 25 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 25 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 26 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 26 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 27 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 27 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 28 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 28 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 29 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 29 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 30 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 30 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 31 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 31 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 32 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 32 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 33 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 33 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 34 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 34 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 35 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 35 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 36 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 36 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 37 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 37 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 38 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 38 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 39 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 39 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 40 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 40 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 41 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 41 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 42 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 42 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 43 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 43 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 44 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 44 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 45 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 45 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 46 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 46 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 47 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 47 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 48 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 48 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 49 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 49 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 50 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 50 | Batchsize: 64 | Steps: 391\n",
      "Epoch 50 | Training checkpoint saved at /fs03/vf38/msyukron/parallel-cnn.pt\n",
      "[GPU1] Epoch 51 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 51 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 52 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 52 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 53 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 53 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 54 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 54 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 55 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 55 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 56 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 56 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 57 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 57 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 58 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 58 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 59 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 59 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 60 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 60 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 61 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 61 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 62 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 62 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 63 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 63 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 64 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 64 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 65 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 65 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 66 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 66 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 67 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 67 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 68 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 68 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 69 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 69 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 70 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 70 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 71 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 71 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 72 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 72 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 73 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 73 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 74 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 74 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 75 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 75 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 76 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 76 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 77 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 77 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 78 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 78 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 79 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 79 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 80 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 80 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 81 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 81 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 82 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 82 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 83 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 83 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 84 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 84 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 85 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 85 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 86 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 86 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 87 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 87 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 88 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 88 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 89 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 89 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 90 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 90 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 91 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 91 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 92 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 92 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 93 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 93 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 94 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 94 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 95 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 95 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 96 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 96 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 97 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 97 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 98 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 98 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 99 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 99 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 100 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 100 | Batchsize: 64 | Steps: 391\n",
      "Epoch 100 | Training checkpoint saved at /fs03/vf38/msyukron/parallel-cnn.pt\n",
      "[GPU1] Epoch 101 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 101 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 102 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 102 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 103 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 103 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 104 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 104 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 105 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 105 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 106 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 106 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 107 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 107 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 108 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 108 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 109 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 109 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 110 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 110 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 111 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 111 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 112 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 112 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 113 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 113 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 114 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 114 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 115 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 115 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 116 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 116 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 117 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 117 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 118 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 118 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 119 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 119 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 120 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 120 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 121 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 121 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 122 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 122 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 123 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 123 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 124 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 124 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 125 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 125 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 126 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 126 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 127 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 127 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 128 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 128 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 129 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 129 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 130 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 130 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 131 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 131 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 132 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 132 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 133 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 133 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 134 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 134 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 135 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 135 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 136 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 136 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 137 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 137 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 138 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 138 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 139 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 139 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 140 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 140 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 141 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 141 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 142 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 142 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 143 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 143 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 144 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 144 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 145 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 145 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 146 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 146 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 147 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 147 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 148 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 148 | Batchsize: 64 | Steps: 391\n",
      "[GPU0] Epoch 149 | Batchsize: 64 | Steps: 391\n",
      "[GPU1] Epoch 149 | Batchsize: 64 | Steps: 391\n",
      "\n",
      "real\t20m6.504s\n",
      "user\t190m17.907s\n",
      "sys\t1m11.527s\n"
     ]
    }
   ],
   "source": [
    "!time python3 /fs03/vf38/msyukron/parallel-cnn.py 150 50 --batch_size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Predicted labels: [6 6 8 0 6 8 6 1 9 6]\n",
      "Actual labels   : [7 9 8 9 6 2 5 2 8 2]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Load the trained model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(128*2*2, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 128*2*2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def load_trained_model(checkpoint_path):\n",
    "    model = SimpleCNN()\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "model = load_trained_model(\"/fs03/vf38/msyukron/parallel-cnn.pt\")\n",
    "\n",
    "# 2. Get 10 new data points\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "testset = torchvision.datasets.CIFAR10(root='/fs03/vf38/msyukron/data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=10, shuffle=True)  # Only one batch of 10 images\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "\n",
    "# 3. Prediction function\n",
    "def predict(model, images):\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    return predicted\n",
    "\n",
    "predicted_labels = predict(model, images)\n",
    "\n",
    "print(\"Predicted labels:\", predicted_labels.numpy())\n",
    "print(\"Actual labels   :\", labels.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Summary: </b>\n",
    "- The DDP version completed the training faster in real time compared to the non-DDP version. This is a good thing and is expected if you have multiple GPUs and the data loading/training can be parallelized efficiently. The real-time reduction suggests that distributing the model across multiple GPUs was effective in speeding up the training.\n",
    "\n",
    "- The user time for the DDP version is almost double compared to the non-DDP version. This makes sense because in DDP, you are using multiple processes (one for each GPU). Each process contributes to the user time. So, if you had two GPUs, for example, you could expect the user time to be roughly double (since both GPUs are doing computational work).\n",
    "\n",
    "- The sys time is slightly higher for the DDP version, which might be due to the overhead of inter-process and GPU-to-GPU communication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: \n",
    "- https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-series/multigpu.py\n",
    "- https://www.youtube.com/watch?v=-LAtx9Q6DA8&ab_channel=PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /fs03/vf38/msyukron/ddp_lightning.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /fs03/vf38/msyukron/ddp_lightning.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class SimpleCNN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(8 * 8 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "def cifar10_dataloader(batch_size, train):\n",
    "    dataset = datasets.CIFAR10(\n",
    "        os.getcwd(), train=train, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "def main():\n",
    "    model = SimpleCNN()\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=2,\n",
    "        accelerator='dp',\n",
    "        max_epochs=5\n",
    "    )\n",
    "    trainer.fit(model, cifar10_dataloader(batch_size=64, train=True))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msyukron/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='dp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='dp')` instead.\n",
      "  f\"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /fs03/vf38/msyukron/cifar-10-python.tar.gz\n",
      "100%|██████████████████████▉| 170205184/170498071 [00:15<00:00, 13941365.70it/s]Extracting /fs03/vf38/msyukron/cifar-10-python.tar.gz to /fs03/vf38/msyukron\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | layer1   | Sequential | 2.4 K \n",
      "1 | layer2   | Sequential | 51.3 K\n",
      "2 | drop_out | Dropout    | 0     \n",
      "3 | fc1      | Linear     | 4.1 M \n",
      "4 | fc2      | Linear     | 10.0 K\n",
      "----------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.643    Total estimated model params size (MB)\n",
      "/home/msyukron/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "170500096it [00:39, 4348661.79it/s] <00:07, 38.39it/s, loss=1.36, v_num=3.19e+7]\n",
      "Epoch 4: 100%|██████| 782/782 [00:17<00:00, 43.59it/s, loss=1.03, v_num=3.19e+7]\n",
      "2023-10-04 19:27:57.049825: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/:/usr/local/cuda/lib::/.singularity.d/libs\n",
      "2023-10-04 19:27:57.049876: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n",
      "real\t2m2.699s\n",
      "user\t1m40.228s\n",
      "sys\t0m18.545s\n"
     ]
    }
   ],
   "source": [
    "!time python3 ddp_lightning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.10\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install pytorch-lightning\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "print(pl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /fs03/vf38/msyukron/ddp_lightning.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /fs03/vf38/msyukron/ddp_lightning.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class SimpleCNN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(8 * 8 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return cifar10_dataloader(batch_size=64, train=True)\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        torch.save(self.state_dict(), \"model.pt\")\n",
    "\n",
    "def cifar10_dataloader(batch_size, train):\n",
    "    dataset = datasets.CIFAR10(\n",
    "        os.getcwd(), train=train, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=4\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "def main():\n",
    "    model = SimpleCNN()\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=2,\n",
    "        strategy='ddp_spawn',\n",
    "        max_epochs=5\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Set SLURM handle signals.\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | layer1   | Sequential | 2.4 K \n",
      "1 | layer2   | Sequential | 51.3 K\n",
      "2 | drop_out | Dropout    | 0     \n",
      "3 | fc1      | Linear     | 4.1 M \n",
      "4 | fc2      | Linear     | 10.0 K\n",
      "----------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.643    Total estimated model params size (MB)\n",
      "/home/msyukron/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /fs03/vf38/msyukron/lightning_logs/version_31899031/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "/home/msyukron/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: num_workers>0, persistent_workers=False, and strategy=ddp_spawn may result in data loading bottlenecks. Consider setting persistent_workers=True (this is a limitation of Python .spawn() and PyTorch)\n",
      "  \"num_workers>0, persistent_workers=False, and strategy=ddp_spawn\"\n",
      "Epoch 4: 100%|██████| 391/391 [00:13<00:00, 28.67it/s, loss=1.01, v_num=3.19e+7]\n",
      "/home/msyukron/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:307: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n",
      "2023-10-04 19:34:22.292651: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/:/usr/local/cuda/lib::/.singularity.d/libs\n",
      "2023-10-04 19:34:22.292697: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n",
      "real\t1m18.317s\n",
      "user\t3m6.357s\n",
      "sys\t1m13.212s\n"
     ]
    }
   ],
   "source": [
    "!time python3 ddp_lightning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:145] . PytorchStreamReader failed reading zip archive: invalid header or archive is corrupted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2f4900cab06c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;31m# reset back to the original position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m                     warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:145] . PytorchStreamReader failed reading zip archive: invalid header or archive is corrupted"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class SimpleCNN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(8 * 8 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "def cifar10_dataloader(batch_size, train):\n",
    "    dataset = datasets.CIFAR10(\n",
    "        os.getcwd(), train=train, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "# Load model\n",
    "model_path = \"model.pt\"\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Load data\n",
    "test_loader = cifar10_dataloader(batch_size=64, train=False)\n",
    "\n",
    "# Predict on test data\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, _ = batch\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
